{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AFID.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM6Sns9LtRatDUcS4rrP3rq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "155c01d3d53b4c88a975ca44567c7542": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_eeef7fe234c24d08a6318a1adeb1d027",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_573b1d4089df444c8846468f48854a89",
              "IPY_MODEL_e2d79e3c4bb24eaa94189e140c9367bc"
            ]
          }
        },
        "eeef7fe234c24d08a6318a1adeb1d027": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "573b1d4089df444c8846468f48854a89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a7a1bc8a3e0a4162b0fa23fd60a7e1a5",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 108857766,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 108857766,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a68d624fba694a4790b870c42744393f"
          }
        },
        "e2d79e3c4bb24eaa94189e140c9367bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c467e5bc1fd54939a3f8951546922854",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 104M/104M [00:30&lt;00:00, 3.55MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_05e032ad24824286be7c742446e5f415"
          }
        },
        "a7a1bc8a3e0a4162b0fa23fd60a7e1a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a68d624fba694a4790b870c42744393f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c467e5bc1fd54939a3f8951546922854": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "05e032ad24824286be7c742446e5f415": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ejnunn/GAN_Research/blob/main/AFID.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rdbbNg67b0m",
        "outputId": "8a5140e4-46bf-4215-da5f-6b43463ebd38"
      },
      "source": [
        "# !rm -r GAN_Research\n",
        "!git clone --quiet https://github.com/ejnunn/GAN_Research.git\n",
        "!pip install torchextractor"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchextractor\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/94/f14591882d0459a626d6aa8ed3699b08e6b79192c26cae87cbd6081cb835/torchextractor-0.3.0-py3-none-any.whl\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from torchextractor) (1.8.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchextractor) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->torchextractor) (3.7.4.3)\n",
            "Installing collected packages: torchextractor\n",
            "Successfully installed torchextractor-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpHEnt2x7gV0"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchextractor as tx\n",
        "import multiprocessing\n",
        "import numpy as np\n",
        "import cv2\n",
        "from scipy import linalg"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102,
          "referenced_widgets": [
            "155c01d3d53b4c88a975ca44567c7542",
            "eeef7fe234c24d08a6318a1adeb1d027",
            "573b1d4089df444c8846468f48854a89",
            "e2d79e3c4bb24eaa94189e140c9367bc",
            "a7a1bc8a3e0a4162b0fa23fd60a7e1a5",
            "a68d624fba694a4790b870c42744393f",
            "c467e5bc1fd54939a3f8951546922854",
            "05e032ad24824286be7c742446e5f415"
          ]
        },
        "id": "_JSrHHVYIis6",
        "outputId": "0009e84c-5028-4ad7-b7e1-398cd5d96cd0"
      },
      "source": [
        "original_model = torchvision.models.inception_v3(pretrained=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-1a9a5a14.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "155c01d3d53b4c88a975ca44567c7542",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=108857766.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1asYMFWG1ss"
      },
      "source": [
        "images1 = torch.rand(128, 3, 299, 299)\n",
        "images2 = torch.rand(128, 3, 299, 299)\n",
        "\n",
        "fid_value1, fid_value2, fid_value3 = calculate_fid(images1, images2, use_multiprocessing=False, batch_size=1)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30g4rQ8QwUlz",
        "outputId": "95c361b5-e5ac-4f5f-b7a6-019dd57a58af"
      },
      "source": [
        "print('fid_value1 =', fid_value1)\n",
        "print('fid_value2 =', fid_value2)\n",
        "print('fid_value3 =', fid_value3)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fid_value1 = 29066.984375359836\n",
            "fid_value2 = 16503.98046884275\n",
            "fid_value3 = 17.05095610770348\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOi0LqIoGl80"
      },
      "source": [
        "# FID Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPGKb-EbHEOV"
      },
      "source": [
        "def get_activations(images, batch_size):\n",
        "    \"\"\"\n",
        "    Calculates activations for last pool layer for all iamges\n",
        "    --\n",
        "        Images: torch.array shape: (N, 3, 299, 299), dtype: torch.float32\n",
        "        batch size: batch size used for inception network\n",
        "    --\n",
        "    Returns: np array shape: (N, 2048), dtype: np.float32\n",
        "    \"\"\"\n",
        "    assert images.shape[1:] == (3, 299, 299), \"Expected input shape to be: (N,3,299,299)\" +\\\n",
        "                                              \", but got {}\".format(images.shape)\n",
        "\n",
        "    num_images = images.shape[0]\n",
        "    original_model = torchvision.models.inception_v3(pretrained=True)\n",
        "    inception_network = tx.Extractor(original_model, ['maxpool1', 'maxpool2', 'avgpool'])\n",
        "    inception_network = to_cuda(inception_network)\n",
        "    inception_network.eval()\n",
        "    n_batches = int(np.ceil(num_images  / batch_size))\n",
        "    inception_activations = np.zeros((num_images, 2048), dtype=np.float32)\n",
        "    for batch_idx in range(n_batches):\n",
        "        start_idx = batch_size * batch_idx\n",
        "        end_idx = batch_size * (batch_idx + 1)\n",
        "\n",
        "        ims = images[start_idx:end_idx]\n",
        "        ims = to_cuda(ims)\n",
        "        model_output, features = inception_network(ims)\n",
        "        act1, act2, act3 = features.values()\n",
        "\n",
        "        act1 = act1.detach().cpu().numpy().flatten()\n",
        "        act1 = np.expand_dims(act1, axis=0)\n",
        "        act2 = act2.detach().cpu().numpy().flatten()\n",
        "        act2 = np.expand_dims(act2, axis=0)\n",
        "        act3 = act3.detach().cpu().numpy().squeeze()\n",
        "        act3 = np.expand_dims(act3, axis=0)\n",
        "        \n",
        "        assert act1.shape == (ims.shape[0], 341056), \"Expexted output shape to be: {}, but was: {}\".format((ims.shape[0], 341056), act1.shape)\n",
        "        assert act2.shape == (ims.shape[0], 235200), \"Expexted output shape to be: {}, but was: {}\".format((ims.shape[0], 235200), act2.shape)\n",
        "        assert act3.shape == (ims.shape[0], 2048), \"Expexted output shape to be: {}, but was: {}\".format((ims.shape[0], 2048), act3.shape)\n",
        "        \n",
        "    return act1, act2, act3"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34-83NzMGkUX"
      },
      "source": [
        "def calculate_activation_statistics(images, batch_size):\n",
        "    \"\"\"Calculates the statistics used by FID\n",
        "    Args:\n",
        "        images: torch.tensor, shape: (N, 3, H, W), dtype: torch.float32 in range 0 - 1\n",
        "        batch_size: batch size to use to calculate inception scores\n",
        "    Returns:\n",
        "        mu:     mean over all activations from the last pool layer of the inception model\n",
        "        sigma:  covariance matrix over all activations from the last pool layer \n",
        "                of the inception model.\n",
        "    \"\"\"\n",
        "    act1, act2, act3 = get_activations(images, batch_size)\n",
        "    \n",
        "    mu1 = np.mean(act1, axis=0)\n",
        "    mu2 = np.mean(act2, axis=0)\n",
        "    mu3 = np.mean(act3, axis=0)\n",
        "\n",
        "    sigma1 = np.cov(act1, rowvar=False)\n",
        "    sigma2 = np.cov(act2, rowvar=False)\n",
        "    sigma3 = np.cov(act3, rowvar=False)\n",
        "    return mu1, mu2, mu3, sigma1, sigma2, sigma3"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1gSAHNXAuv0"
      },
      "source": [
        "# Modified from: https://github.com/bioinf-jku/TTUR/blob/master/fid.py\n",
        "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
        "    \"\"\"Numpy implementation of the Frechet Distance.\n",
        "    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n",
        "    and X_2 ~ N(mu_2, C_2) is\n",
        "            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
        "            \n",
        "    Stable version by Dougal J. Sutherland.\n",
        "    Params:\n",
        "    -- mu1 : Numpy array containing the activations of the pool_3 layer of the\n",
        "             inception net ( like returned by the function 'get_predictions')\n",
        "             for generated samples.\n",
        "    -- mu2   : The sample mean over activations of the pool_3 layer, precalcualted\n",
        "               on an representive data set.\n",
        "    -- sigma1: The covariance matrix over activations of the pool_3 layer for\n",
        "               generated samples.\n",
        "    -- sigma2: The covariance matrix over activations of the pool_3 layer,\n",
        "               precalcualted on an representive data set.\n",
        "    Returns:\n",
        "    --   : The Frechet Distance.\n",
        "    \"\"\"\n",
        "\n",
        "    mu1 = np.atleast_1d(mu1)\n",
        "    mu2 = np.atleast_1d(mu2)\n",
        "\n",
        "    sigma1 = np.atleast_2d(sigma1)\n",
        "    sigma2 = np.atleast_2d(sigma2)\n",
        "\n",
        "    assert mu1.shape == mu2.shape, \"Training and test mean vectors have different lengths\"\n",
        "    assert sigma1.shape == sigma2.shape, \"Training and test covariances have different dimensions\"\n",
        "\n",
        "    diff = mu1 - mu2\n",
        "    # product might be almost singular\n",
        "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
        "    if not np.isfinite(covmean).all():\n",
        "        msg = \"fid calculation produces singular product; adding %s to diagonal of cov estimates\" % eps\n",
        "        warnings.warn(msg)\n",
        "        offset = np.eye(sigma1.shape[0]) * eps\n",
        "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
        "\n",
        "    # numerical error might give slight imaginary component\n",
        "    if np.iscomplexobj(covmean):\n",
        "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
        "            m = np.max(np.abs(covmean.imag))\n",
        "            raise ValueError(\"Imaginary component {}\".format(m))\n",
        "        covmean = covmean.real\n",
        "\n",
        "    tr_covmean = np.trace(covmean)\n",
        "\n",
        "    return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OHm7P8JHheO"
      },
      "source": [
        "def calculate_fid(images1, images2, use_multiprocessing, batch_size):\n",
        "    \"\"\" Calculate FID between images1 and images2\n",
        "    Args:\n",
        "        images1: np.array, shape: (N, H, W, 3), dtype: np.float32 between 0-1 or np.uint8\n",
        "        images2: np.array, shape: (N, H, W, 3), dtype: np.float32 between 0-1 or np.uint8\n",
        "        use_multiprocessing: If multiprocessing should be used to pre-process the images\n",
        "        batch size: batch size used for inception network\n",
        "    Returns:\n",
        "        FID (scalar)\n",
        "    \"\"\"\n",
        "    # images1 = preprocess_images(images1, use_multiprocessing)\n",
        "    # images2 = preprocess_images(images2, use_multiprocessing)\n",
        "    mu11, mu12, mu13, sigma11, sigma12, sigma13 = calculate_activation_statistics(images1, batch_size)\n",
        "    mu21, mu22, mu23, sigma21, sigma22, sigma23 = calculate_activation_statistics(images2, batch_size)\n",
        "    fid1 = calculate_frechet_distance(mu11, sigma11, mu21, sigma21)\n",
        "    fid2 = calculate_frechet_distance(mu12, sigma12, mu22, sigma22)\n",
        "    fid3 = calculate_frechet_distance(mu13, sigma13, mu23, sigma23)\n",
        "    return fid1, fid2, fid3"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "El2R0k9PIWsJ"
      },
      "source": [
        "def preprocess_image(im):\n",
        "    \"\"\"Resizes and shifts the dynamic range of image to 0-1\n",
        "    Args:\n",
        "        im: np.array, shape: (H, W, 3), dtype: float32 between 0-1 or np.uint8\n",
        "    Return:\n",
        "        im: torch.tensor, shape: (3, 299, 299), dtype: torch.float32 between 0-1\n",
        "    \"\"\"\n",
        "    assert im.shape[0] == 3\n",
        "    assert len(im.shape) == 3\n",
        "    if im.dtype == np.uint8:\n",
        "        im = im.astype(np.float32) / 255\n",
        "    im = cv2.resize(im, (299, 299))\n",
        "    im = np.rollaxis(im, axis=2)\n",
        "    im = torch.from_numpy(im)\n",
        "    assert im.max() <= 1.0\n",
        "    assert im.min() >= 0.0\n",
        "    assert im.dtype == torch.float32\n",
        "    assert im.shape == (3, 299, 299)\n",
        "\n",
        "    return im"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umds19pPIXkf"
      },
      "source": [
        "def preprocess_images(images, use_multiprocessing):\n",
        "    \"\"\"Resizes and shifts the dynamic range of image to 0-1\n",
        "    Args:\n",
        "        images: np.array, shape: (N, H, W, 3), dtype: float32 between 0-1 or np.uint8\n",
        "        use_multiprocessing: If multiprocessing should be used to pre-process the images\n",
        "    Return:\n",
        "        final_images: torch.tensor, shape: (N, 3, 299, 299), dtype: torch.float32 between 0-1\n",
        "    \"\"\"\n",
        "    if use_multiprocessing:\n",
        "        with multiprocessing.Pool(multiprocessing.cpu_count()) as pool:\n",
        "            jobs = []\n",
        "            for im in images:\n",
        "                job = pool.apply_async(preprocess_image, (im,))\n",
        "                jobs.append(job)\n",
        "            final_images = torch.zeros(images.shape[0], 3, 299, 299)\n",
        "            for idx, job in enumerate(jobs):\n",
        "                im = job.get()\n",
        "                final_images[idx] = im#job.get()\n",
        "    else:\n",
        "        final_images = torch.stack([preprocess_image(im) for im in images], dim=0)\n",
        "    assert final_images.shape == (images.shape[0], 3, 299, 299)\n",
        "    assert final_images.max() <= 1.0\n",
        "    assert final_images.min() >= 0.0\n",
        "    assert final_images.dtype == torch.float32\n",
        "    return final_images"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dl86Q_X4J93l"
      },
      "source": [
        "def to_cuda(elements):\n",
        "    \"\"\"\n",
        "    Transfers elements to cuda if GPU is available\n",
        "    Args:\n",
        "        elements: torch.tensor or torch.nn.module\n",
        "        --\n",
        "    Returns:\n",
        "        elements: same as input on GPU memory, if available\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return elements.cuda()\n",
        "    return elements"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdI5PKJjEdyw"
      },
      "source": [
        "# Random code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5NKKygt7qf3",
        "outputId": "1d59501f-b7af-4d84-9db4-424a867d04f0"
      },
      "source": [
        "model = tx.Extractor(original_model, ['maxpool1', 'maxpool2', 'avgpool'])\n",
        "model.eval()\n",
        "dummy = torch.rand(1, 3, 299, 299)\n",
        "model_output, features = model(dummy)\n",
        "feature_shapes = {name: f.shape for name, f in features.items()}\n",
        "print(feature_shapes)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'maxpool1': torch.Size([1, 64, 73, 73]), 'maxpool2': torch.Size([1, 192, 35, 35]), 'avgpool': torch.Size([1, 2048, 1, 1])}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Q21np45u7o7",
        "outputId": "bd7d9f41-794f-4452-d269-1c94bb631f2d"
      },
      "source": [
        "for name, f in features.items():\n",
        "  print(name, f.flatten().shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "maxpool1 torch.Size([341056])\n",
            "maxpool2 torch.Size([235200])\n",
            "avgpool torch.Size([2048])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyqOosUu7waS"
      },
      "source": [
        "[x for x in tx.list_module_names(model) if x.find('pool') != -1]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}